{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"other_features.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"6qTSsjGEoJgb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# https://www.kaggle.com/c/ngsa-2018"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MY6fIc6-af8x","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import re\n","import itertools\n","\n","from gensim import corpora, models, similarities\n","import networkx as nx\n","from networkx.algorithms.connectivity import local_edge_connectivity\n","from networkx.algorithms.connectivity import build_auxiliary_edge_connectivity\n","from networkx.algorithms.flow import build_residual_network\n","import igraph\n","\n","import nltk\n","from nltk.corpus import stopwords\n","stopwords = stopwords.words(\"english\")\n","#nltk.download('punkt') # for tokenization\n","\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","from sklearn import preprocessing"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cKml7k20oJgi","colab_type":"text"},"cell_type":"markdown","source":["# Import data"]},{"metadata":{"id":"JlSiGwixawpI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["nodes_header = [\"id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"]\n","node_info = pd.read_csv('node_information.csv',names=nodes_header)\n","node_info.set_index(\"id\", inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XG2ugScrhXpk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"90f5733a-56e0-4996-9a8d-d5dffc894811"},"cell_type":"code","source":["node_info.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>title</th>\n","      <th>authors</th>\n","      <th>journal</th>\n","      <th>abstract</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1001</th>\n","      <td>2000</td>\n","      <td>compactification geometry and duality</td>\n","      <td>Paul S. Aspinwall</td>\n","      <td>NaN</td>\n","      <td>these are notes based on lectures given at tas...</td>\n","    </tr>\n","    <tr>\n","      <th>1002</th>\n","      <td>2000</td>\n","      <td>domain walls and massive gauged supergravity p...</td>\n","      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n","      <td>Class.Quant.Grav.</td>\n","      <td>we point out that massive gauged supergravity ...</td>\n","    </tr>\n","    <tr>\n","      <th>1003</th>\n","      <td>2000</td>\n","      <td>comment on metric fluctuations in brane worlds</td>\n","      <td>Y.S. Myung, Gungwon Kang</td>\n","      <td>NaN</td>\n","      <td>recently ivanov and volovich hep-th 9912242 cl...</td>\n","    </tr>\n","    <tr>\n","      <th>1004</th>\n","      <td>2000</td>\n","      <td>moving mirrors and thermodynamic paradoxes</td>\n","      <td>Adam D. Helfer</td>\n","      <td>Phys.Rev.</td>\n","      <td>quantum fields responding to moving mirrors ha...</td>\n","    </tr>\n","    <tr>\n","      <th>1005</th>\n","      <td>2000</td>\n","      <td>bundles of chiral blocks and boundary conditio...</td>\n","      <td>J. Fuchs, C. Schweigert</td>\n","      <td>NaN</td>\n","      <td>proceedings of lie iii clausthal july 1999 var...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      year                                              title  \\\n","id                                                              \n","1001  2000              compactification geometry and duality   \n","1002  2000  domain walls and massive gauged supergravity p...   \n","1003  2000     comment on metric fluctuations in brane worlds   \n","1004  2000         moving mirrors and thermodynamic paradoxes   \n","1005  2000  bundles of chiral blocks and boundary conditio...   \n","\n","                          authors            journal  \\\n","id                                                     \n","1001            Paul S. Aspinwall                NaN   \n","1002  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n","1003     Y.S. Myung, Gungwon Kang                NaN   \n","1004               Adam D. Helfer          Phys.Rev.   \n","1005      J. Fuchs, C. Schweigert                NaN   \n","\n","                                               abstract  \n","id                                                       \n","1001  these are notes based on lectures given at tas...  \n","1002  we point out that massive gauged supergravity ...  \n","1003  recently ivanov and volovich hep-th 9912242 cl...  \n","1004  quantum fields responding to moving mirrors ha...  \n","1005  proceedings of lie iii clausthal july 1999 var...  "]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"id":"d72DBlrPg9RT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["names = [\"id1\",\"id2\",\"category\"]\n","train = pd.read_csv('training_set.txt', names=names,delimiter=\" \")\n","train[\"index\"] = train[\"id1\"].astype(str) + \"|\" + train[\"id2\"].astype(str)\n","train.set_index(\"index\", inplace=True)\n","train_size=615512\n","\n","name = [\"id1\",\"id2\"]\n","test = pd.read_csv('testing_set.txt',names=name,delimiter=\" \")\n","#train[\"index\"] = train[\"id1\"].astype(str) + \"|\" + train[\"id2\"].astype(str)\n","#train.set_index(\"index\", inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xx14QPVPoJgv","colab_type":"text"},"cell_type":"markdown","source":["# Preprossing"]},{"metadata":{"id":"W-wUf-d7oJgw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# this part is to do some cleaning on titles and abstract\n","\n","stemmer = nltk.porter.PorterStemmer()\n","\n","def cleaning(text):\n","    # split and lower the text\n","    text= text.apply(lambda sentence:  \"\".join((char if char.isalpha() else \" \") for char in sentence).lower().split() )\n","    # remove stopwords\n","    text = text.apply(lambda words : [word for word in words if word not in stopwords])\n","    # stemmer\n","    text = text.apply(lambda words : [stemmer.stem(word) for word in words])\n","    return text"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kG9w04_IoJgy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["                                     # -----clean title----- #\n","titles = cleaning(node_info['title'])\n","node_info['clean_titles'] = titles\n","\n","                                     # -----clean abstract---- #\n","abstracts = cleaning(node_info['abstract'])\n","node_info['clean_abstracts'] = abstracts\n","\n","                                     # -----split authors---- #\n","authors = node_info['authors'].astype(str).str.split(',')\n","node_info['split_authors'] = authors"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sBpAKVQ1cwmi","colab_type":"text"},"cell_type":"markdown","source":["# Generate Features"]},{"metadata":{"id":"n63wFLPWoJg1","colab_type":"text"},"cell_type":"markdown","source":["### overlapping words in title / overlapping words in abstract / number of common author"]},{"metadata":{"id":"wugBt9UooJg3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"1cb12c29-7c3a-4771-e7b0-45b60274698d"},"cell_type":"code","source":["def get_overlap(id1,id2,text,split=True):\n","    text_1 = node_info.at[id1, text]\n","    text_2 = node_info.at[id2, text]\n","    count = len(set(text_1).intersection(set(text_2)))\n","    return count\n","\n","                                # --- overlapping words in title --- #\n","print('start computing overlapping words in title')\n","train['overlapping_words_in_title'] = 0\n","for index, row in train.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['overlapping_words_in_title'] = get_overlap(id1, id2, 'clean_titles')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start computing overlapping words in title\n","536.% of the task done\n","552.% of the task done\n","568.% of the task done\n","584.% of the task done\n","601.% of the task done\n","617.% of the task done\n"],"name":"stdout"}]},{"metadata":{"id":"dj8rTtEboJg5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"a67fdfa5-a072-4e49-824d-279017496f69"},"cell_type":"code","source":["                            # --- overlapping words in abstract --- #\n","print('start computing overlapping words in abstract')\n","train['overlapping_words_in_abstract'] = 0\n","for index, row in train.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['overlapping_words_in_abstract'] = get_overlap(int(id1),int(id2), 'clean_abstracts')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)\n","    \n","                                 # --- common authors --- #\n","print('start computing common authors')\n","train['common_author'] = 0\n","for index, row in train.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['common_author'] = get_overlap(int(id1),int(id2), 'split_authors')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start computing overlapping words in abstract\n","633.% of the task done\n","649.% of the task done\n","666.% of the task done\n","682.% of the task done\n","698.% of the task done\n","714.% of the task done\n","start computing common authors\n","731.% of the task done\n","747.% of the task done\n","763.% of the task done\n","779.% of the task done\n","796.% of the task done\n","812.% of the task done\n"],"name":"stdout"}]},{"metadata":{"id":"4jD2Iw9EoJg9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"eceaf79b-1b97-4b14-9536-9b276e8f4330"},"cell_type":"code","source":["                                # --- overlapping words in title --- #\n","print('start computing overlapping words in title')\n","test['overlapping_words_in_title'] = 0\n","for index, row in test.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['overlapping_words_in_title'] = get_overlap(int(id1),int(id2), 'clean_titles')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)\n","    \n","                              # --- overlapping words in abstract --- #\n","print('start computing overlapping words in abstract')\n","test['overlapping_words_in_abstract'] = 0\n","for index, row in test.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['overlapping_words_in_abstract'] = get_overlap(int(id1),int(id2), 'clean_abstracts')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)\n","    \n","                                 # --- common authors --- #\n","print('start computing common authors')\n","test['common_author'] = 0\n","for index, row in test.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['common_author'] = get_overlap(int(id1),int(id2), 'split_authors')\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start computing overlapping words in title\n","start computing overlapping words in abstract\n","828.% of the task done\n","start computing common authors\n"],"name":"stdout"}]},{"metadata":{"id":"7T8wadWQoJhA","colab_type":"text"},"cell_type":"markdown","source":["### temporal distance between two papers"]},{"metadata":{"id":"RFR4tjSloJhB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"185182a8-038a-4d97-dec0-37d7b789609d"},"cell_type":"code","source":["print('start computing temporal distance between papers')\n","train['tem_distance'] = 0\n","for index, row in train.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['tem_distance'] = int(node_info['year'][id1]) - int(node_info['year'][id2])\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start computing temporal distance between papers\n","844.% of the task done\n","861.% of the task done\n","877.% of the task done\n","893.% of the task done\n","909.% of the task done\n","926.% of the task done\n"],"name":"stdout"}]},{"metadata":{"id":"yuwBVJg4oJhF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}]},"outputId":"dc820250-8b5f-489f-ab2f-0afd1ae471c0"},"cell_type":"code","source":["print('start computing temporal distance between papers')\n","test['tem_distance'] = 0\n","for index, row in test.iterrows():\n","    id1, id2 = row['id1'], row['id2']\n","    row['tem_distance'] = int(node_info['year'][id1]) - int(node_info['year'][id2])\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start computing temporal distance between papers\n","942.% of the task done\n"],"name":"stdout"}]},{"metadata":{"id":"q3K7YBlfoJhL","colab_type":"text"},"cell_type":"markdown","source":["###  feature TFIDF in abstract"]},{"metadata":{"id":"zw6vyukQoJhN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["vectorizer = TfidfVectorizer(stop_words=\"english\")\n","# each row is a node in the order of node_info\n","features_TFIDF = vectorizer.fit_transform(node_info['abstract'])\n","\n","#features_TFIDF.toarray()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"902rWX3Si0Tf","colab_type":"text"},"cell_type":"markdown","source":["### cosine similarity in abstract"]},{"metadata":{"id":"p3y_pXMloJhQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{},{},{},{}]},"outputId":"52d7b30f-be1d-4c18-95e7-2cfc655c742a"},"cell_type":"code","source":["# mark*: the result is quiet different between using \"abstract\" to compute tf-idf and using \"clean_abstracts\",\n","# \"clean abstract have been removed stopwords and done stemming\".\n","# below is used \"clean_abstracts\", but I am not sure if it would be better than using \"abstract\".\n","\n","dictionary = corpora.Dictionary(abstracts)\n","tfidf = models.TfidfModel(dictionary=dictionary)\n","\n","def get_tf_idf_encoding(index):\n","    abstract = node_info.at[index, \"clean_abstracts\"]\n","    #abstract = abstract.split(\" \")\n","    abstract = dictionary.doc2bow(abstract)\n","    count = tfidf[[abstract]]\n","    return count[0]\n","\n","def my_norm(f):\n","    ans = 0  \n","    for (k, v) in f:\n","        ans += v**2    \n","    return np.sqrt(ans)\n","    \n","def cosine_distance(id1, id2):\n","    f1 = get_tf_idf_encoding(id1)\n","    f2 = get_tf_idf_encoding(id2)\n","    denom = my_norm(f1) * my_norm(f2)\n","    f1 = dict(f1)\n","    f2 = dict(f2)\n","    \n","    ans = 0.0\n","    \n","    for k, v in f1.items():\n","        if k in f2.keys():\n","            ans += v * f2[k]\n","    return ans/denom\n","\n","print('start compute the cosine similarity in abstract')\n","i = 0\n","for index, row in train.iterrows():\n","    id1, id2 = int(row[\"id1\"]), int(row[\"id2\"]) \n","    train.set_value(index, \"cosine_distance\", cosine_distance(id1, id2))\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)\n","    \n","print('start compute the cosine similarity in abstract')\n","i = 0\n","for index, row in test.iterrows():\n","    id1, id2 = int(row[\"id1\"]), int(row[\"id2\"]) \n","    test.set_value(index, \"cosine_distance\", cosine_distance(id1, id2))\n","    i += 1\n","    if int(i) % 100000 == 0:\n","        print(str(100.0*i/train_size)[:4]+\"% of the task done\", flush=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start compute the cosine similarity in abstract\n"],"name":"stdout"},{"output_type":"stream","text":["/home/xiaofeng/.local/lib/python3.5/site-packages/ipykernel_launcher.py:39: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"],"name":"stderr"},{"output_type":"stream","text":["16.2% of the task done\n","32.4% of the task done\n","48.7% of the task done\n","64.9% of the task done\n","81.2% of the task done\n","97.4% of the task done\n","start compute the cosine similarity in abstract\n"],"name":"stdout"},{"output_type":"stream","text":["/home/xiaofeng/.local/lib/python3.5/site-packages/ipykernel_launcher.py:48: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"],"name":"stderr"}]},{"metadata":{"id":"4QwE2vlLoJhU","colab_type":"text"},"cell_type":"markdown","source":["### edge connectivity"]},{"metadata":{"id":"fOyhvKwboJhU","colab_type":"text"},"cell_type":"markdown","source":["H = build_auxiliary_edge_connectivity(di_network_graph)\n","R = build_residual_network(H, 'capacity')\n","di_connectivity = dict.fromkeys(di_network_graph, dict())\n","for u, v in itertools.combinations(di_network_graph, 2):\n","    k = local_edge_connectivity(di_network_graph, u, v, auxiliary=H, residual=R)\n","    di_connectivity[u][v] = k\n","\n","H = build_auxiliary_edge_connectivity(un_network_graph)\n","R = build_residual_network(H, 'capacity')\n","un_connectivity = dict.fromkeys(un_network_graph, dict())\n","for u, v in itertools.combinations(un_network_graph, 2):\n","    k = local_edge_connectivity(un_network_graph, u, v, auxiliary=H, residual=R)\n","    un_connectivity[u][v] = k\n","            \n","# edge connectivity\n","di_connectivities = [di_connectivity[vs[str(id1)], di_connectivity[str(id2)]] for id1, id2 in zip(id1, id2)]\n","un_connectivities = [un_connectivity[vs[str(id1)], un_connectivity[str(id2)]] for id1, id2 in zip(id1, id2)]"]},{"metadata":{"id":"jz11awvzoJhV","colab_type":"text"},"cell_type":"markdown","source":["train['di_connectivities'] = di_connectivities\n","train['un_connectivities'] = un_connectivities"]},{"metadata":{"id":"OHzuI6EtoJhV","colab_type":"text"},"cell_type":"markdown","source":["### in the same cluster or not"]},{"metadata":{"id":"TnMW3TQGoJhX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# no idea how to do it"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EcerqhwWoJhZ","colab_type":"text"},"cell_type":"markdown","source":["### same journal"]},{"metadata":{"id":"Ldt2IPq0oJhb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["id1 = train['id1'].values\n","id2 = train['id2'].values\n","journal_1 = np.array(node_info.loc[id1,'journal'])\n","journal_2 = np.array(node_info.loc[id2,'journal'])\n","na_journal = np.array((journal_1==\"NaN\") | (journal_2==\"NaN\")).astype(int)\n","same_journal = np.array((journal_1!=\"NaN\") & (journal_2!=\"NaN\") & (journal_1==journal_2)).astype(int)\n","diff_journal = np.array((journal_1!=\"NaN\") & (journal_2!=\"NaN\") & (journal_1!=journal_2)).astype(int)\n","\n","train['na_journal'] = na_journal\n","train['same_journal'] = same_journal\n","train['diff_journal'] = diff_journal"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Kdm6OjNWoJhc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["id1_t = test['id1'].values\n","id2_t = test['id2'].values\n","journal_1_t = np.array(node_info.loc[id1_t,'journal'])\n","journal_2_t = np.array(node_info.loc[id2_t,'journal'])\n","na_journal_t = np.array((journal_1_t==\"NaN\") | (journal_2_t==\"NaN\")).astype(int)\n","same_journal_t = np.array((journal_1_t!=\"NaN\") & (journal_2_t!=\"NaN\") & (journal_1_t==journal_2_t)).astype(int)\n","diff_journal_t = np.array((journal_1_t!=\"NaN\") & (journal_2_t!=\"NaN\") & (journal_1_t!=journal_2_t)).astype(int)\n","\n","test['na_journal'] = na_journal_t\n","test['same_journal'] = same_journal_t\n","test['diff_journal'] = diff_journal_t"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NrySIQOwoJhe","colab_type":"text"},"cell_type":"markdown","source":["### cosine similarity in titles"]},{"metadata":{"id":"XEUXnnI4oJhf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}