{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# Keras imports\n",
    "import keras\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# For modelling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# From string manipulation\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"/Users/.../semester_2/deep_learning/nlp_project/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors \\n' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # Dict for storing words and similarities\n",
    "        self.sim_words = {}\n",
    "        \n",
    "        # For each element in the dict keys\n",
    "        for i in list(self.word2vec.keys()):\n",
    "            # Compute the similarity between the two vectors\n",
    "            cos = self.score(w, i)\n",
    "            \n",
    "            # When the lenght of the dictionary is lower than 5.. auto update\n",
    "            if len(self.sim_words) < K:\n",
    "                self.sim_words[i] = cos\n",
    "                \n",
    "            # If the computed score is greater than the minimal best score\n",
    "            elif cos > min(self.sim_words.values()):\n",
    "                # Delete the key to the minimum best score word\n",
    "                del self.sim_words[min(self.sim_words, \n",
    "                                          key=self.sim_words.get)] \n",
    "                # update dict\n",
    "                self.sim_words[i] = cos\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        return sorted(self.sim_words, key=self.sim_words.get, reverse=True)\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        w1_vec, w2_vec = self.word2vec[w1], self.word2vec[w2]\n",
    "        \n",
    "        # Simple cosine calculations\n",
    "        self.cos = np.dot(w1_vec, w2_vec)/(np.linalg.norm(w1_vec)*np.linalg.norm(w2_vec))\n",
    "        return self.cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors \n",
      "\n",
      "cat dog 0.6716836662792491\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'pooches', 'Dogs', 'doggies']\n",
      "['paris', 'france', 'Paris', 'parisian', 'london']\n",
      "['germany', 'austria', 'europe', 'german', 'berlin']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "    \n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                sent_vec = [self.w2v.word2vec[w] for w in sent if w in self.w2v.word2vec]\n",
    "                \n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                sent_vec = [self.w2v.word2vec[w]*self.idf_dict[w] for w in sent \n",
    "                            if (w in self.w2v.word2vec) and (w in self.idf_dict)]\n",
    "        \n",
    "            sentemb.append(np.mean(sent_vec, axis=0))   \n",
    "        \n",
    "        if len(sentences) is 1:\n",
    "            return sentemb[0]\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        self.sim_sent, query = {}, self.encode([s], idf)\n",
    "        \n",
    "        # Normalizing the vector.\n",
    "        query = query/np.linalg.norm(query, 2)\n",
    "            \n",
    "        for sent in sentences:\n",
    "            # Rule out the case where it is the same sentence. \n",
    "            if sent != s:\n",
    "                sent_vec = self.encode([sent], idf)\n",
    "\n",
    "                # Normalizing the vector & Computing the cosine similarity.\n",
    "                sent_vec = sent_vec/np.linalg.norm(sent_vec)\n",
    "                cos = np.dot(sent_vec, query)/(np.linalg.norm(sent_vec)*np.linalg.norm(query))\n",
    "\n",
    "                # When the lenght of the dictionary is lower than 5.. auto update\n",
    "                if len(self.sim_sent) < K:\n",
    "                    self.sim_sent[' '.join(sent)] = cos\n",
    "\n",
    "                # If the computed score is greater than the minimal best score\n",
    "                elif cos > min(self.sim_sent.values()):\n",
    "                    # Delete the key to the minimum best score word\n",
    "                    del self.sim_sent[min(self.sim_sent, \n",
    "                                              key=self.sim_sent.get)] \n",
    "                    # update dict\n",
    "                    self.sim_sent[' '.join(sent)] = cos\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        similarity_dict = sorted(self.sim_sent, key=self.sim_sent.get, reverse=True)\n",
    "        \n",
    "        # Printing out.\n",
    "        print(\"The {} most similar sentences to '{}' are: \\n \".format(K, ' '.join(s)))\n",
    "        for i in range(K):\n",
    "            print('{}) {}'.format(i, similarity_dict[i]))\n",
    "        print('\\n')\n",
    "        return None\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        s1_vec, s2_vec = self.encode([s1], idf), self.encode([s2], idf)\n",
    "        \n",
    "        # Need to normalize both sentence vectors\n",
    "        s1_vec, s2_vec = s1_vec/np.linalg.norm(s1_vec), s2_vec/np.linalg.norm(s2_vec)\n",
    "        sim = np.dot(s1_vec.T, s2_vec)\n",
    "    \n",
    "        print(\"Sentence similarity between '{}' and '{}' is {} \\n\".format(' '.join(s1), ' '.join(s2), sim))\n",
    "        return None\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                # Default set to 0 or add 1 if present\n",
    "                self.idf_dict[w] = self.idf_dict.get(w, 0) + 1\n",
    "        \n",
    "        # Update all values of the dict by the idf value max(1, np.log10(doc_size/term_freq))\n",
    "        self.idf_dict = dict([(\n",
    "            k, max(1, np.log10(len(sentences)/v))\n",
    "        ) \n",
    "                    for (k, v) in self.idf_dict.items()])\n",
    "        \n",
    "        return self.idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors \n",
      "\n",
      "The 5 most similar sentences to '1 smiling african american boy .' are: \n",
      " \n",
      "0) an african american man smiling .\n",
      "1) a little african american boy and girl looking up .\n",
      "2) an afican american woman standing behind two small african american children .\n",
      "3) an african american man is sitting .\n",
      "4) a girl in black hat holding an african american baby .\n",
      "\n",
      "\n",
      "Sentence similarity between '1 man singing and 1 man playing a saxophone in a concert .' and '10 people venture out to go crosscountry skiing .' is 0.5726258859719606 \n",
      "\n",
      "The 5 most similar sentences to '1 smiling african american boy .' are: \n",
      " \n",
      "0) an african american man smiling .\n",
      "1) an african american man is sitting .\n",
      "2) a little african american boy and girl looking up .\n",
      "3) an afican american woman standing behind two small african american children .\n",
      "4) a girl in black hat holding an african american baby .\n",
      "\n",
      "\n",
      "Sentence similarity between '1 man singing and 1 man playing a saxophone in a concert .' and '10 people venture out to go crosscountry skiing .' is 0.47514508753687806 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with open(PATH_TO_DATA+'/sentences.txt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sent = line.rstrip().split()\n",
    "        sentences.append(sent)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf_dict = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf=True)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading 50000 vectors for url: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec \n",
      "\n",
      "Finished downloading 50000 vectors for url: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def download_to_dict(url, n_first=50000):\n",
    "    re = urllib.request.urlopen(url)\n",
    "    word_embed_dict = {}\n",
    "    for i, l in enumerate(re):\n",
    "        # Limit download size to default value\n",
    "        if i > n_first: break\n",
    "        elif i is 0: pass\n",
    "        else:\n",
    "            word, embed = l.decode('utf-8').split(' ',maxsplit=1)\n",
    "            # Convert to a numpy array\n",
    "            embed = np.array([float(i) for i in embed.split(' ') if i is not '\\n'])\n",
    "            word_embed_dict[word] = embed\n",
    "    print('Finished downloading {} vectors for url: {} \\n'.format(len(word_embed_dict), url))\n",
    "    return word_embed_dict\n",
    "\n",
    "eng_word_dict = download_to_dict('https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec')\n",
    "french_word_dict = download_to_dict('https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "common_keys = list(set(eng_word_dict.keys()) & set(french_word_dict.keys()))\n",
    "\n",
    "def create_mat(word_dict, common_keys):\n",
    "    vect_word_list = []\n",
    "    for k in common_keys:\n",
    "        vect = [k] + list(word_dict[k])\n",
    "        vect_word_list.append(np.array(vect))\n",
    "    # Return the vertical stacking of the list of numpy array. \n",
    "    return np.vstack(vect_word_list)\n",
    "\n",
    "Y, X = create_mat(eng_word_dict, common_keys), create_mat(french_word_dict, common_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "X_words, Y_words = X[:, 0].reshape((X.shape[0], 1)), Y[:, 0].reshape((Y.shape[0], 1))\n",
    "Y, X = Y[:, 1:].astype(float), X[:, 1:].astype(float)\n",
    "\n",
    "\n",
    "U, sig, V_T = np.linalg.svd(np.dot(Y, X.T))\n",
    "W = np.dot(U, V_T)\n",
    "X_W = np.dot(W, X)\n",
    "\n",
    "# Add the word column to X_W\n",
    "X_W, Y = np.hstack((X_words, X_W)), np.hstack((Y_words, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode french_to_eng is False \n",
      "\n",
      "For 'automobile', the 20 most similar words are: \n",
      "0) 'automobile', similarity: 3.60475435273381\n",
      "1) 'automobiles', similarity: 2.7896719427395182\n",
      "2) 'automotive', similarity: 2.548950634310399\n",
      "3) 'auto', similarity: 2.336638025740818\n",
      "4) 'motor', similarity: 2.258243193359415\n",
      "5) 'peugeot', similarity: 2.1870605957286005\n",
      "6) 'suv', similarity: 2.0405373875784356\n",
      "7) 'car', similarity: 2.0299109279698118\n",
      "8) 'chrysler', similarity: 1.9407918065894374\n",
      "9) 'bugatti', similarity: 1.9138707825451826\n",
      "10) 'citroën', similarity: 1.8975421240174004\n",
      "11) 'chevrolet', similarity: 1.8890310202169673\n",
      "12) 'motors', similarity: 1.886694080433701\n",
      "13) 'roadster', similarity: 1.846398040556708\n",
      "14) 'buick', similarity: 1.8323967516072264\n",
      "15) 'bicycle', similarity: 1.81310302140496\n",
      "16) 'volkswagen', similarity: 1.7796050990111425\n",
      "17) 'maserati', similarity: 1.7795186108185654\n",
      "18) 'lancia', similarity: 1.7692075618333871\n",
      "19) 'cars', similarity: 1.7659284474350994\n",
      "\n",
      "\n",
      "Mode french_to_eng is True \n",
      "\n",
      "For 'maison', the 10 most similar words are: \n",
      "0) 'maison', similarity: 3.671807783488302\n",
      "1) 'hôtel', similarity: 2.064458596015319\n",
      "2) 'rue', similarity: 1.874173677508755\n",
      "3) 'château', similarity: 1.8108214294088543\n",
      "4) 'mère', similarity: 1.7698949467489413\n",
      "5) 'châteaux', similarity: 1.7514885586645605\n",
      "6) 'histoire', similarity: 1.742438707120702\n",
      "7) 'chambre', similarity: 1.7167169769662851\n",
      "8) 'historique', similarity: 1.7082469887869811\n",
      "9) 'maître', similarity: 1.6959392505991728\n",
      "\n",
      "\n",
      "{'french_to_eng': False, 'automobile': {1.846398040556708: 'roadster', 1.8890310202169673: 'chevrolet', 2.0405373875784356: 'suv', 1.8323967516072264: 'buick', 1.886694080433701: 'motors', 1.9407918065894374: 'chrysler', 2.336638025740818: 'auto', 3.60475435273381: 'automobile', 1.7692075618333871: 'lancia', 2.7896719427395182: 'automobiles', 1.8975421240174004: 'citroën', 1.7796050990111425: 'volkswagen', 1.7795186108185654: 'maserati', 1.9138707825451826: 'bugatti', 1.81310302140496: 'bicycle', 2.258243193359415: 'motor', 2.0299109279698118: 'car', 1.7659284474350994: 'cars', 2.1870605957286005: 'peugeot', 2.548950634310399: 'automotive'}} {'french_to_eng': True, 'maison': {1.7167169769662851: 'chambre', 2.064458596015319: 'hôtel', 3.671807783488302: 'maison', 1.742438707120702: 'histoire', 1.6959392505991728: 'maître', 1.7082469887869811: 'historique', 1.874173677508755: 'rue', 1.7698949467489413: 'mère', 1.8108214294088543: 'château', 1.7514885586645605: 'châteaux'}}\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def nn_words(words, X_W, Y, french_to_eng=True, n=10, verbose=True):\n",
    "    # Check that we have a list. Make a dict containing all the word under consideration.\n",
    "    if type(words) is str:\n",
    "        words = [words]\n",
    "    nn_dict = {}\n",
    "    nn_dict['french_to_eng'] = french_to_eng\n",
    "    \n",
    "    for word in words:\n",
    "        # Make key pointing to a dict for word in consideration\n",
    "        nn_dict[word] = {}\n",
    "        \n",
    "        if french_to_eng is True:\n",
    "            # Get and Normalize vector\n",
    "            word_v = X_W[np.array(np.where(X_W == word))[0], 1:].astype(float)\n",
    "            word_v = word_v/np.linalg.norm(word_v)\n",
    "            \n",
    "            for i in np.arange(Y.shape[0]):\n",
    "                # Get the minimum sim value up to know when dict is not empty\n",
    "                if len(nn_dict[word]) is not 0:\n",
    "                    best_min_sim = min(list(nn_dict[word].keys()))\n",
    "                sim = np.dot(\n",
    "                word_v, Y[i, 1:].astype(float)\n",
    "                )[0]\n",
    "                \n",
    "                \n",
    "                # When dict did not fill up to user-provided limit\n",
    "                if len(nn_dict[word]) < n:\n",
    "                    nn_dict[word][sim] = Y[i, 0]\n",
    "                    \n",
    "                # If filled up, start selection\n",
    "                elif sim > best_min_sim:\n",
    "                    del nn_dict[word][best_min_sim]\n",
    "                    nn_dict[word][sim] = Y[i, 0]\n",
    "                    \n",
    "                else:pass\n",
    "                    \n",
    "        else:\n",
    "            # Get and normalize vector. Same as previous, starting matrix is simply Y instead of WX.\n",
    "            word_v = Y[np.array(np.where(Y == word))[0], 1:].astype(float)\n",
    "            word_v = word_v/np.linalg.norm(word_v)\n",
    "            \n",
    "            for i in np.arange(X_W.shape[0]):\n",
    "                if len(nn_dict[word]) is not 0:\n",
    "                    best_min_sim = min(list(nn_dict[word].keys()))\n",
    "                sim = np.dot(\n",
    "                word_v, X_W[i, 1:].astype(float)\n",
    "                )[0]\n",
    "                \n",
    "                if len(nn_dict[word]) < n:\n",
    "                    nn_dict[word][sim] = X_W[i, 0]\n",
    "                    \n",
    "                elif sim > best_min_sim:\n",
    "                    del nn_dict[word][best_min_sim]\n",
    "                    nn_dict[word][sim] = X_W[i, 0]\n",
    "                else:pass\n",
    "        \n",
    "    if verbose is True: \n",
    "        # For printing results\n",
    "        print(\"Mode french_to_eng is {} \\n\".format(french_to_eng))\n",
    "        for word in words:\n",
    "            print(\"For '{}', the {} most similar words are: \".format(word, n))\n",
    "            for i, key in enumerate(sorted(nn_dict[word], reverse=True)):\n",
    "                print(\"{}) '{}', similarity: {}\".format(i, nn_dict[word][key], key))\n",
    "            print('\\n')\n",
    "            \n",
    "    # Returns dict, whose keys are considered words and values dict to the n most similar words.\n",
    "    return nn_dict\n",
    "\n",
    "print(nn_words('automobile', X_W, Y, french_to_eng=False, n=20), nn_words('maison', X_W, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "list_data_file = ['stsa.fine.dev', 'stsa.fine.test.X', 'stsa.fine.train']\n",
    "\n",
    "def load_sst_data(list_data_file):\n",
    "    dev_set = {'label':[], 'sentence':[]}\n",
    "    test_set = {'sentence':[]}\n",
    "    train_set = {'label':[], 'sentence':[]}\n",
    "    for i, f in enumerate(list_data_file):\n",
    "        \n",
    "        with open(PATH_TO_DATA+'/SST/'+f, 'r') as data:\n",
    "            for l in data:\n",
    "                if i is 0: \n",
    "                    label, sentence = l.split(maxsplit=1)\n",
    "                    dev_set['label'].append(label), dev_set['sentence'].append(sentence)\n",
    "                elif i is 1:\n",
    "                    # Test set has no label\n",
    "                    test_set['sentence'].append(l)\n",
    "                else:\n",
    "                    label, sentence = l.split(maxsplit=1)\n",
    "                    train_set['label'].append(label), train_set['sentence'].append(sentence)\n",
    "    return pd.DataFrame(train_set), pd.DataFrame(dev_set), pd.DataFrame(test_set)\n",
    "\n",
    "# Get data\n",
    "df_train, df_dev, df_test = load_sst_data(list_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Function for adding the bov features to the df\n",
    "def add_bov_features(df, s2v, weighted=False):\n",
    "    sent_embeds = []\n",
    "    \n",
    "    if weighted is False:\n",
    "        for i in range(df.shape[0]):\n",
    "            # For each row, calculate the sentence embedding. \n",
    "            embed = s2v.encode([df.loc[i, 'sentence']])\n",
    "            sent_embeds.append(embed)\n",
    "\n",
    "        embeds_df = pd.DataFrame(sent_embeds)\n",
    "\n",
    "        # update dataframe (concatenation)\n",
    "        df = pd.concat([df, embeds_df], axis=1)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Now do the BoV embedding, each weighted by the tf-idf of the word\n",
    "        for i in range(df.shape[0]):\n",
    "            # For each row, calculate the sentence embedding.\n",
    "            embed = s2v.encode([df.loc[i, 'sentence']], idf=True)\n",
    "            sent_embeds.append(embed)\n",
    "\n",
    "        embeds_df = pd.DataFrame(sent_embeds)\n",
    "\n",
    "        # update dataframe (concatenation)\n",
    "        df = pd.concat([df, embeds_df], axis=1)\n",
    "        return df        \n",
    "        \n",
    "\n",
    "def clean_s(s, translate_table):\n",
    "    s = s.translate(translate_table)\n",
    "    return s\n",
    "\n",
    "translate_table = str.maketrans(dict.fromkeys(punctuation, None))\n",
    "\n",
    "df_train['sentence'] = df_train['sentence'].apply(clean_s, translate_table=translate_table)  \n",
    "df_dev['sentence'] = df_dev['sentence'].apply(clean_s, translate_table=translate_table)  \n",
    "df_test['sentence'] = df_test['sentence'].apply(clean_s, translate_table=translate_table)  \n",
    "\n",
    "\n",
    "# Taking the same dfs but with weighted bov.\n",
    "dfs_idf_dict = s2v.build_idf(sentences_dfs)\n",
    "\n",
    "df_dev_noweight, df_test_noweight, df_train_noweight = add_bov_features(df_dev, s2v), add_bov_features(df_test, s2v), add_bov_features(df_train, s2v)\n",
    "df_dev_withweight, df_test_withweight, df_train_withweight = add_bov_features(df_dev, s2v, weighted=True), add_bov_features(df_test, s2v, weighted=True), add_bov_features(df_train, s2v, weighted=True)\n",
    "\n",
    "\n",
    "# Need to rebuild the idf dict for the bov\n",
    "sentences_dfs = pd.concat([df_train_withweight[\"sentence\"], df_test_withweight[\"sentence\"],\n",
    "                           df_dev_withweight['sentence']], axis=0).tolist()\n",
    "sentences_dfs = [s.strip().split() for s in sentences_dfs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.30067883895131087\n",
      "Accuracy on dev. set: 0.3169845594913715\n",
      "\n",
      "For weighted BoV,\n",
      "Accuracy on train set: 0.3026685393258427\n",
      "Accuracy on dev. set: 0.3151680290644868\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "\n",
    "\n",
    "# TYPE CODE HERE\n",
    "log_clf = LogisticRegression()\n",
    "param_grid = {'C':np.arange(0.01, 2, 0.1)}\n",
    "gridcv = GridSearchCV(log_clf, param_grid)\n",
    "gridcv.fit(df_dev_noweight.iloc[:, 2:], df_dev_noweight.loc[:, 'label'])\n",
    "\n",
    "# Now fitting the best parameters\n",
    "log_clf = LogisticRegression(**gridcv.best_params_)\n",
    "log_fit = log_clf.fit(df_train_noweight.iloc[:, 2:], \n",
    "                      df_train_noweight.loc[:, 'label'])\n",
    "\n",
    "accuracy_train = accuracy_score(log_fit.predict(df_train_noweight.iloc[:, 2:]), \n",
    "                                df_train_noweight.loc[:, 'label'])\n",
    "\n",
    "\n",
    "preds = log_fit.predict(df_dev_noweight.iloc[:, 2:])\n",
    "accuracy_dev = accuracy_score(df_dev_noweight.loc[:, 'label'], preds)\n",
    "print('Accuracy on train set: {}\\nAccuracy on dev. set: {}\\n'.format(accuracy_train, \n",
    "                                                                   accuracy_dev))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression(**gridcv.best_params_)\n",
    "log_fit = log_clf.fit(df_train_withweight.iloc[:, 2:], df_train_withweight.loc[:, 'label'])\n",
    "accuracy_train_weighted = accuracy_score(log_fit.predict(df_train_withweight.iloc[:, 2:]),  \n",
    "                                                        df_train_withweight.loc[:, 'label'])\n",
    "\n",
    "preds = log_fit.predict(df_dev_withweight.iloc[:, 2:])\n",
    "accuracy_dev_weighted = accuracy_score(df_dev_withweight.loc[:, 'label'], preds)\n",
    "                                        \n",
    "print('For weighted BoV,\\nAccuracy on train set: {}\\nAccuracy on dev. set: {}'.format(accuracy_train_weighted, \n",
    "                                                           accuracy_dev_weighted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "test_preds = pd.DataFrame.from_dict({'preds':log_fit.predict(df_test_noweight.iloc[:, 1:])})\n",
    "test_preds.to_csv(PATH_TO_DATA+'/logreg_bov_y_test_sst.txt', header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# Preprocess text into sequences\n",
    "sentences_dfs = df_train['sentence'].tolist() + df_dev['sentence'].tolist() + df_test['sentence'].tolist()\n",
    "\n",
    "def longest_sentence_voc_size(sent_list):\n",
    "    # Simple function for returning the biggest sentence length and the voc size\n",
    "    max_sent = 10\n",
    "    for s in sent_list:\n",
    "        if len(s.split()) > max_sent:\n",
    "            max_sent = len(s.split())\n",
    "        else: pass\n",
    "        \n",
    "    # Now get the voc size\n",
    "    whole_text = ' '.join(sent_list)\n",
    "    voc_size = len(Counter(whole_text.split()))\n",
    "    \n",
    "    return max_sent, voc_size\n",
    "\n",
    "max_sent, voc_size = longest_sentence_voc_size(sentences_dfs)\n",
    "\n",
    "def get_sentences_sequences(df_list, voc_size, max_sent):\n",
    "    df_padded_sequences = []\n",
    "    # Get the sentence into a properly padded sequence\n",
    "    for df in df_list:\n",
    "        # One hot encoding\n",
    "        df['one_hot'] = df['sentence'].apply(lambda x: text.one_hot(x, n=voc_size))\n",
    "        \n",
    "        # Pad the sequences\n",
    "        padded_sequences = sequence.pad_sequences(df['one_hot'], max_sent)\n",
    "        df_padded_sequences.append(padded_sequences)\n",
    "        \n",
    "    return df_padded_sequences\n",
    "\n",
    "train_seq, dev_seq, test_seq = get_sentences_sequences([df_train, \n",
    "                                                        df_dev, \n",
    "                                                        df_test], voc_size, max_sent)\n",
    "\n",
    "n_neurons, n_classes = 100, len(np.unique(df_train.loc[:, 'label']))\n",
    "embed_dim = 300\n",
    "\n",
    "# Build the model\n",
    "bonus_model = Sequential()\n",
    "bonus_model.add(Embedding(voc_size, 300))\n",
    "bonus_model.add(LSTM(n_neurons, return_sequences=True, \n",
    "                     dropout_U=0.5, dropout_W=0.5))\n",
    "bonus_model.add(LSTM(n_neurons))\n",
    "\n",
    "# Going to the dense part. \n",
    "bonus_model.add(Dropout(0.4))\n",
    "bonus_model.add(Dense(n_neurons, activation='relu'))\n",
    "bonus_model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compiling\n",
    "bonus_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/5\n",
      "8544/8544 [==============================] - 39s 5ms/step - loss: 1.5694 - acc: 0.2747 - val_loss: 1.5837 - val_acc: 0.2625\n",
      "Epoch 2/5\n",
      "8544/8544 [==============================] - 34s 4ms/step - loss: 1.4585 - acc: 0.3632 - val_loss: 1.4278 - val_acc: 0.3633\n",
      "Epoch 3/5\n",
      "8544/8544 [==============================] - 34s 4ms/step - loss: 1.2616 - acc: 0.4675 - val_loss: 1.4194 - val_acc: 0.3878\n",
      "Epoch 4/5\n",
      "8544/8544 [==============================] - 34s 4ms/step - loss: 1.1102 - acc: 0.5393 - val_loss: 1.5091 - val_acc: 0.3678\n",
      "Epoch 5/5\n",
      "8544/8544 [==============================] - 38s 4ms/step - loss: 0.9947 - acc: 0.6009 - val_loss: 1.4981 - val_acc: 0.3660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4243bbc18>"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonus_model.fit(train_seq, target_train, epochs=5, batch_size=60, \n",
    "                validation_data=(dev_seq, target_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bonus_model.predict(test_seq)\n",
    "preds_labels = preds.argmax(axis=-1)\n",
    "pd.DataFrame(preds_labels).to_csv(PATH_TO_DATA+'LSTM_bov_y_test_sst.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "# TYPE CODE HERE\n",
    "\n",
    "\n",
    "df_train, df_dev, df_test = load_sst_data(list_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "punctuation = punctuation + '\\n'\n",
    "trans_table = str.maketrans(dict.fromkeys(punctuation))\n",
    "\n",
    "def process_sentence(sentence, trans_table):\n",
    "    # For text preprocessing.\n",
    "    sentence = sentence.translate(trans_table)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# Use voc size after having pre process the sentence\n",
    "def voc_size(sentence_list):\n",
    "    voc = []\n",
    "    max_sentence_length = 10\n",
    "    # Simply appending a list of unique words if not in list. \n",
    "    for s in sentence_list:\n",
    "        tokenized_s = s.split()\n",
    "        # Keep track of the largest sentence\n",
    "        if len(tokenized_s) > 10: \n",
    "            max_sentence_length = len(tokenized_s)\n",
    "        for w in s.split():\n",
    "            if w not in voc:\n",
    "                voc.append(w)\n",
    "            else:pass\n",
    "    # Returns integer, size of list of unique words.\n",
    "    return len(voc)\n",
    "    \n",
    "def return_with_padded_seq(df, maxlen):\n",
    "    # returns the dataframe with the padded one-hot sequences.\n",
    "    return pd.concat(\n",
    "        [df, \n",
    "         pd.DataFrame(sequence.pad_sequences(df[\"one_hot_seq\"], maxlen=maxlen))\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "def max_seq_size(df):\n",
    "    df[\"seq_len\"] = df[\"one_hot_seq\"].apply(lambda x:len(x))\n",
    "    max_seq = max(df[\"seq_len\"])\n",
    "    del df[\"seq_len\"]\n",
    "    return max_seq\n",
    "\n",
    "# Process train sentences, get vocab size and max sent length, encode in one hot and get padded vect.\n",
    "df_train[\"sentence\"] = df_train[\"sentence\"].apply(process_sentence, trans_table=trans_table)\n",
    "train_voc_size = voc_size(df_train[\"sentence\"])\n",
    "df_train[\"one_hot_seq\"] = df_train[\"sentence\"].apply(lambda x: text.one_hot(x, n=train_voc_size))\n",
    "\n",
    "# Get the maximum lenght of sequence.\n",
    "max_train_seq_size = max_seq_size(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "df_train = return_with_padded_seq(df_train, maxlen=max_train_seq_size)\n",
    "\n",
    "# Same for dev, using the train numbers. \n",
    "df_dev[\"sentence\"] = df_dev[\"sentence\"].apply(process_sentence, trans_table=trans_table)\n",
    "df_dev[\"one_hot_seq\"] = df_dev[\"sentence\"].apply(lambda x: text.one_hot(x, n=train_voc_size))\n",
    "df_dev = return_with_padded_seq(df_dev, maxlen=max_train_seq_size)\n",
    "\n",
    "# Finally simply preprocess test_df\n",
    "df_test[\"sentence\"] = df_test[\"sentence\"].apply(process_sentence, trans_table=trans_table)\n",
    "df_test[\"one_hot_seq\"] = df_test[\"sentence\"].apply(lambda x: text.one_hot(x, n=train_voc_size))\n",
    "df_test = return_with_padded_seq(df_test, maxlen=max_train_seq_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = train_voc_size # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          526432    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 551,589\n",
      "Trainable params: 551,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'RMSprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 9s 1ms/step - loss: 1.5745 - acc: 0.2715 - val_loss: 1.5726 - val_acc: 0.2552\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 7s 859us/step - loss: 1.5521 - acc: 0.2962 - val_loss: 1.5232 - val_acc: 0.3497\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 8s 929us/step - loss: 1.4343 - acc: 0.3724 - val_loss: 1.4271 - val_acc: 0.3778\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.3030 - acc: 0.4197 - val_loss: 1.4027 - val_acc: 0.3769\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.2103 - acc: 0.4417 - val_loss: 1.4086 - val_acc: 0.3787\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 9s 1ms/step - loss: 1.1218 - acc: 0.4706 - val_loss: 1.4428 - val_acc: 0.3660\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "x_train, x_val = df_train.iloc[:, 3:], df_dev.iloc[:, 3:]\n",
    "y_train, y_val = to_categorical(df_train.iloc[:, 0]), to_categorical(df_dev.iloc[:, 0])\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, \n",
    "                    nb_epoch=n_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def get_label(preds):\n",
    "    # Simple function to get column label for the predicted label.\n",
    "    df_preds = pd.DataFrame(preds)\n",
    "    df_preds['label'] = 0\n",
    "    for i in range(df_preds.shape[0]):\n",
    "        index = list(df_preds.loc[i, :]).index(max(list(df_preds.loc[i, :])))\n",
    "        df_preds.loc[i, 'label'] = index\n",
    "    return df_preds\n",
    "\n",
    "df_preds = get_label(model.predict(df_test.iloc[:, 2:]))\n",
    "df_preds['label'].to_csv(PATH_TO_DATA+\"logreg_lstm_y_test_sst.txt\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# Starting the tokenization of data\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), \n",
    "               nmax=500000)\n",
    "df_train, df_dev, df_test = load_sst_data(list_data_file)\n",
    "\n",
    "# Concatenating the dfs together only for the embedding matrix\n",
    "df_sentences = pd.concat([df_train, df_dev, df_test])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_sentences.loc[:, 'sentence'])\n",
    "sequences = tokenizer.texts_to_sequences(df_sentences.loc[:, 'sentence'])\n",
    "\n",
    "# Some functions for proper padding and initialize the weight matrix\n",
    "\n",
    "def max_seq(list_seq):\n",
    "    for l in list_seq:\n",
    "        yield len(l)\n",
    "\n",
    "def load_embedding_matrix(embedding_dim, tokenizer):\n",
    "    # Embedding matrix is of size (voc, embed_dim).\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, \n",
    "                                 embedding_dim))\n",
    "    for w, v in tokenizer.word_index.items():\n",
    "        try:\n",
    "            word_vector = w2v.word2vec[w]\n",
    "            embedding_matrix[v] = word_vector\n",
    "        except:\n",
    "            # Vector is initialized to zeros if not present in dict.\n",
    "            continue\n",
    "    return embedding_matrix\n",
    "\n",
    "# Get the biggest sequence\n",
    "max_seq_length = max(max_seq(sequences))\n",
    "\n",
    "# Padding the sequences\n",
    "padded_sequences = sequence.pad_sequences(sequences, \n",
    "                                          maxlen=max_seq_length)\n",
    "\n",
    "# Get training and test data\n",
    "x_train, y_train = padded_sequences[:df_train.shape[0]], to_categorical(df_train.loc[:, 'label'])\n",
    "x_dev, y_dev = padded_sequences[df_train.shape[0]:df_train.shape[0]+df_dev.shape[0]], to_categorical(df_dev.loc[:, 'label'])\n",
    "x_test = padded_sequences[df_train.shape[0]+df_dev.shape[0]:df_train.shape[0]+df_dev.shape[0]+df_test.shape[0]]\n",
    "\n",
    "# Make the matrix for the embedding layer\n",
    "embed_dim = w2v.word2vec['hey'].shape[0]\n",
    "embed_mat = load_embedding_matrix(embed_dim, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(len(tokenizer.word_index)+1, embed_dim,\n",
    "                            weights=[embed_mat], \n",
    "                            trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add the pretrained embedding layer\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(80, dropout_W=0.2, \n",
    "               dropout_U=0.2, \n",
    "               return_sequences=True))\n",
    "model.add(LSTM(80))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Use softmax instead of sigmoid\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "8544/8544 [==============================] - 23s 3ms/step - loss: 1.4260 - acc: 0.3626 - val_loss: 1.3516 - val_acc: 0.3987\n",
      "Epoch 2/10\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.3156 - acc: 0.4336 - val_loss: 1.3121 - val_acc: 0.4278\n",
      "Epoch 3/10\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.2798 - acc: 0.4439 - val_loss: 1.3060 - val_acc: 0.4124\n",
      "Epoch 4/10\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.2506 - acc: 0.4525 - val_loss: 1.2760 - val_acc: 0.4169\n",
      "Epoch 5/10\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.2285 - acc: 0.4609 - val_loss: 1.3300 - val_acc: 0.4214\n",
      "Epoch 6/10\n",
      "8544/8544 [==============================] - 18s 2ms/step - loss: 1.2003 - acc: 0.4855 - val_loss: 1.2730 - val_acc: 0.4423\n",
      "Epoch 7/10\n",
      "8544/8544 [==============================] - 20s 2ms/step - loss: 1.1819 - acc: 0.4902 - val_loss: 1.2432 - val_acc: 0.4559\n",
      "Epoch 8/10\n",
      "8544/8544 [==============================] - 20s 2ms/step - loss: 1.1594 - acc: 0.5029 - val_loss: 1.2447 - val_acc: 0.4623\n",
      "Epoch 9/10\n",
      "8544/8544 [==============================] - 20s 2ms/step - loss: 1.1328 - acc: 0.5204 - val_loss: 1.2569 - val_acc: 0.4569\n",
      "Epoch 10/10\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.1050 - acc: 0.5284 - val_loss: 1.2629 - val_acc: 0.4605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177aafef0>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(x_train, y_train, batch_size=bs, \n",
    "          epochs=10, validation_data=(x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get predictions out in csv format\n",
    "\n",
    "preds = model.predict(x_test)\n",
    "preds_labels = preds.argmax(axis=-1)\n",
    "\n",
    "preds_labels = pd.DataFrame(preds_labels)\n",
    "preds_labels.to_csv(PATH_TO_DATA+'DEEP_LSTM_y_test_sst.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
