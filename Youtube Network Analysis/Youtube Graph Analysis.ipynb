{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can find these stuffs in this part:\n",
    "1) __preliminary graph statistical analysis__\n",
    "#1 total number of edges \n",
    "#2 nodes\n",
    "#3 strongest connected component \n",
    "#4 average degree of the node \n",
    "#5 average path length\n",
    "#6 diameter \n",
    "#7 clustering coefficient\n",
    "\n",
    "2) __detecting the strength of the video categories by finding out which categories of videos had the__\n",
    "#8 highest number of videos\n",
    "#9 highest number of views \n",
    "#10 highest comments\n",
    "\n",
    "3) __detecting Influential Video Uploaders for finding out which uploader with had the__ \n",
    "#11 highest number of videos\n",
    "#12 highest number of views \n",
    "#13 highest subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path,'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        txt = list(reader)\n",
    "    txt = [element[0].split('\\t') for element in txt]\n",
    "    return txt\n",
    "\n",
    "data_path = \"/home/silvia/文档/DSBA/NGSA/Final Project/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt0, txt1, txt2, txt3 is the 1st depth, 2nd depth, 3th depth and 4th depth BFS respectively.\n",
    "# here we used the data collected at 2007-03-02, about 40k distinct videos in all 3 depth datasets.\n",
    "# There are 10324 raws of data.\n",
    "\n",
    "txt1 = load_data(os.path.join(data_path, '0302/0.txt'))\n",
    "txt2 = load_data(os.path.join(data_path, '0302/1.txt'))\n",
    "txt3 = load_data(os.path.join(data_path, '0302/2.txt'))\n",
    "data =  txt1 + txt2 + txt3\n",
    "total = data[0:5000] # using just 5k videos....limited by the computer computing capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names for each column: \n",
    "# [video ID, uploader, age, category, length, views, rate, ratings, comments]\n",
    "node_info = [element[0:9] for element in total]\n",
    "\n",
    "# the rest columns are related video id of source video\n",
    "related_id = [element[10:] for element in total]\n",
    "\n",
    "# returns all the source video ids in the dataset, they are nodes in the graph\n",
    "ids = [element[0] for element in total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "nodes_pair = list(combinations(ids,2))\n",
    "# np.savetxt('nodes_pair.txt', nodes_pair)\n",
    "\n",
    "# then we gonna see whether the nodes pair connect with each other or not.\n",
    "# for a nodes pair (a,b), if nodes b appears in the related_id of nodes a, then a and b are connected.\n",
    "\n",
    "id1 = [ x for x,y in nodes_pair] # the source video id\n",
    "id2 = [ y for x,y in nodes_pair] # the target video id\n",
    "\n",
    "# create a id:related_id dictionary \n",
    "related_id_dict = {k:v for k,v in zip(ids,related_id)}\n",
    "\n",
    "# to decide whether two nodes in a node pair connect or not\n",
    "connect = [1 if y not in related_id_dict[x] else 0 for x,y in nodes_pair]\n",
    "pair_connect_dict = {k:v for k,v in zip(nodes_pair,connect)} # build a nodes_pair:connect dictionary\n",
    "\n",
    "edges = [nodes_pair for nodes_pair in nodes_pair if pair_connect_dict[nodes_pair] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the graph, here we use igraph\n",
    "\n",
    "import igraph\n",
    "\n",
    "# create raw empty undirected graph\n",
    "g = igraph.Graph(directed=False)\n",
    "\n",
    "# add vertices\n",
    "g.add_vertices(ids)\n",
    "\n",
    "# add edges\n",
    "g.add_edges(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because it would be easier to use networkx to analyse some features, so we also build a graph using networkx\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "#add nodes\n",
    "G.add_nodes_from(ids)\n",
    "\n",
    "#add edges\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of edges(using igraph)\n",
    "print(len(edges))\n",
    "\n",
    "# __output__: 12480152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to see the degree distribution (using igraph)\n",
    "print(g.degree_distribution(bin_width=2))\n",
    "\n",
    "# __output__:\n",
    "# N = 5000, mean +- sd: 4992.0608 +- 6.0542\n",
    "# Each * represents 19 items\n",
    "# [4970, 4972):  (2)\n",
    "# [4972, 4974):  (4)\n",
    "# [4974, 4976): * (24)\n",
    "# [4976, 4978): * (19)\n",
    "# [4978, 4980): ****** (116)\n",
    "# [4980, 4982): ************* (248)\n",
    "# [4982, 4984): ************* (253)\n",
    "# [4984, 4986): ************** (282)\n",
    "# [4986, 4988): ************* (255)\n",
    "# [4988, 4990): ************** (283)\n",
    "# [4990, 4992): ***************** (336)\n",
    "# [4992, 4994): *********************** (442)\n",
    "# [4994, 4996): ********************************* (633)\n",
    "# [4996, 4998): ********************************************************** (1117)\n",
    "# [4998, 5000): *************************************************** (986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_components = g.components(mode='strong')\n",
    "print(strong_components)\n",
    "# because the videos we chose are very related with other videos in the dataset, so there is just one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the information of communities \n",
    "C = g.community_infomap()\n",
    "\n",
    "# to see the number of communities\n",
    "print(len(C))\n",
    "\n",
    "# to see the size of each communities\n",
    "for n in range(0,len(C)):\n",
    "    print('Community nº', n, 'size:', len(C[n]))\n",
    "    \n",
    "# __output__:\n",
    "# 1\n",
    "# Community nº 0 size: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diameter: the longest path of the shortest paths between any two nodes.\n",
    "print(g.diameter(directed=True))\n",
    "\n",
    "# __output__: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average shortest path\n",
    "\n",
    "print(nx.average_shortest_path_length(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster coefficient\n",
    "# transitivity_undirected is 3 x (# of triangles) / (# of connected triplets)\n",
    "\n",
    "print(g.transitivity_undirected())\n",
    "\n",
    "# __output__ = 0.9986124578666629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graph\n",
    "\n",
    "#z=g.layout('fr')\n",
    "igraph.plot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Analysis__:\n",
    "\n",
    "__Community__: Because the videos we chose are very related with other videos in the dataset, so there is just one cluster\n",
    "\n",
    "__Diameter__: Diameter is small in this graph, which means all the nodes in this graph connect very tightly.\n",
    "\n",
    "__Cluster coefficient__: A clustering coefficient is a measure of the degree to which nodes in a graph tend to cluster together. The result is very high, which is line with real world for in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterised by a relatively high density of ties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Video characteristics analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sum([element[3:4] for element in total],[])\n",
    "cat_list = list(set(categories)) # return a distinct category list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which category have the highest number of videos\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(categories)) # to see the number of videos in each category\n",
    "\n",
    "# plot a histgram to shwo the result\n",
    "plt.figure(figsize=(5,5))\n",
    "labels, values = zip(*Counter(categories).items())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 0.5\n",
    "\n",
    "plt.bar(indexes, values, width, color='Thistle')\n",
    "plt.xticks(indexes + width * 0.5, labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstly, we define a function to calculate the number of views by certain classifier. Actually, I want badly to use Tableau to do so.\n",
    "\n",
    "def Sum(clf,item,Type):\n",
    "    Sum = sum([int(y) for x,y in zip(clf,Type) if x == item])\n",
    "    return Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which category have the highest unmber of views\n",
    "\n",
    "views = sum([element[5:6] for element in total],[])\n",
    "v_sum = [Sum(categories,cat, views) for cat in cat_list]\n",
    "print('_Number of views by category_:','\\n',{k:v for k,v in zip(cat_list,v_sum)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which category have the highest number of comments\n",
    "\n",
    "comments = sum([element[7:8] for element in total],[])\n",
    "c_sum = [Sum(categories,cat,comments) for cat in cat_list]\n",
    "print('_Number of comments by category_:','\\n',{k:v for k,v in zip(cat_list,c_sum)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploader analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which uploader with highest number of videos\n",
    "\n",
    "uploaders = sum([element[1:2] for element in total],[])\n",
    "uploaders_list = list(set(uploaders))\n",
    "print('The uploaders who has the highest number of videos is:',max(Counter(uploaders_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which uploader with highest number of views\n",
    "\n",
    "uploader_v_sum = [Sum(uploaders,upl,views) for upl in uploaders_list]\n",
    "print('The uploaders who has the highest number of views is:',\n",
    "      {k:v for k,v in zip(uploader_v_sum,uploaders_list)}[max(uploader_v_sum)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which uploader with the highest number of comments\n",
    "\n",
    "uploader_c_sum = [Sum(uploaders,upl,comments) for upl in uploaders_list]\n",
    "print('The uploaders who has the highest number of comments is:',\n",
    "      {k:v for k,v in zip(uploader_c_sum,uploaders_list)}[max(uploader_c_sum)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
